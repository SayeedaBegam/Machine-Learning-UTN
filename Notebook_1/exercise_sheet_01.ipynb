{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Basic Module\n",
    "Florian Walter, Tobias JÃ¼lg, Pierre Krack\n",
    "\n",
    "## General Information About Implementation Assignments\n",
    "We will use the Jupyter Notebook for our implementation exercises. The task description will be provided in the notebook. The code is also run in the notebook. However, the implementation itself is done in additional files, which are imported in the notebook. Please provide your code only in the marked positions of the Python files. The content of a Python file could, for example, look similar as shown below:\n",
    "```python\n",
    "def f():\n",
    "    ########################################################################\n",
    "    # YOUR CODE\n",
    "    # TODO: Implement this function\n",
    "    ########################################################################\n",
    "    pass\n",
    "    ########################################################################\n",
    "    # END OF YOUR CODE\n",
    "    ########################################################################\n",
    "```\n",
    "To complete the exercise, remove the `pass` command and only use space inside the `YOUR CODE` block to provide a solution. Other lines within the file may not be changed in order to deliver a valid solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_loading import load_data, compute_summary\n",
    "from perceptron import Perceptron, fit\n",
    "from plotting import plot_decision_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: The Perceptron\n",
    "The Perceptron is one of the simplest forms of a neural network, often referred to as a single-layer binary classifier. It was introduced by Frank Rosenblatt in the late 1950s and can be thought of as a building block for larger neural networks. Essentially, the Perceptron is an algorithm that, given an input vector, can decide whether it belongs to one class or another.\n",
    "\n",
    "We will use the Perceptron as an example to shortly introduce all major parts of a standard machine learning pipeline, including\n",
    "- Loading the dataset\n",
    "- Visualizing the dataset and prediction outcomes\n",
    "- Implementing the model and auxilary training code\n",
    "- Evaluating the model's performance\n",
    "\n",
    "\n",
    "## Loading the Dataset\n",
    "[Pandas](https://pandas.pydata.org/) is widely used in ML. It has more functionality than we need here but we can still use it to visualize the data.\n",
    "> **Task 1** Implement the `load_data()` function in [`data_loading.py`](./data_loading.py), then execute the next cell to display it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m display(data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "data: pd.DataFrame = load_data(\"data.csv\")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can, see each row has an $x$ and a $y$ value as well as a label.\n",
    "Now imagine you get a new datapoint $(x, y) = (-0.8, -0.3)$, which originates from the same underlying distribution as the dataset displayed above.\n",
    "Which class does it belong to, $-1$ or $1$? This is the classification problem.\n",
    "\n",
    "The table above seems to suggest that the elements with negative $x$ and $y$ values belong the $-1$ class and vice versa.\n",
    "But maybe the small excerpt shown in above does not tell the full story.\n",
    "We can further inspect the dataset by computing summary statistics.\n",
    "\n",
    "> **Task 2** Implement the `compute_summary()` function in [`data_loading.py`](./data_loading.py). It should return a dictionary with keys `cnt`, `avg` and `std`, which contain summary statistics for the value counts, the mean value and the standard deviation, grouped by label. Use the pandas functions intended for this purpose. You can find a quick tutorial [here](https://pandas.pydata.org/docs/getting_started/intro_tutorials/06_calculate_statistics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = compute_summary(data)\n",
    "print(\"Counts\")\n",
    "display(summary[\"cnt\"])\n",
    "print(\"Averages\")\n",
    "display(summary[\"avg\"])\n",
    "print(\"Standard deviation\")\n",
    "display(summary[\"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now have a better overview of the data and it is easy to say with high confidence that $(x, y) = (-0.8, -0.3)$ belongs to the class $-1$.\n",
    "However this is still not enough: what about the data point $(x, y) = (-1.4, 1.5)$?\n",
    "Seems like it could easily belong both to $-1$ and to $1$.\n",
    "Let us visualize the data with [matplotlib](https://matplotlib.org/). Run the cell below and see if you find it easier to classify the data point $(-1.4, 1.5)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m label: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtab:blue\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtab:orange\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mscatter(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39marray,\n\u001b[0;32m      3\u001b[0m             data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39marray,\n\u001b[0;32m      4\u001b[0m             color \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(color)(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "color = lambda label: \"tab:blue\" if label == -1 else \"tab:orange\"\n",
    "plt.scatter(data[\"x\"].array,\n",
    "            data[\"y\"].array,\n",
    "            color = np.vectorize(color)(data[\"label\"]),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting labels with an artificial neuron\n",
    "Plotting the data can help us decide where the point should go.\n",
    "However we still can not automate the process, you have to decide visually for each point using this approach, we want the computer to make that decision for us.\n",
    "Additionally, the visualization approach breaks if your dataset has rows with 10 values per row instead of two.\n",
    "\n",
    "We can solve this problem with the Perceptron.\n",
    "\n",
    ">**Task 3** Open [`perceptron.py`](./perceptron.py) and implement the `initialize_weights()`, `activation()`, `predict_forloop()` and `predict_vectorized()` methods.\n",
    ">- Initialize the weights using numpy's random module\n",
    ">- Use the sign function as activation function:\n",
    ">$$\n",
    "\\textrm{sign}(x) = \\begin{cases}\n",
    "1,& \\text{if } x \\geq 1\\\\\n",
    "-1,& \\text{if } x < 0\n",
    "\\end{cases}\n",
    ">$$\n",
    ">- The `predict_forloop` method should use a python for loop and resemble the following formula:\n",
    ">$$A\\left(\\sum_{i=0}^1 w_i*x_i +b\\right)$$\n",
    ">- The `predict_vectorized` method should use numpy functions and resemble the following formula:\n",
    ">$$A\\left(w^Tx+b\\right)$$\n",
    "\n",
    "Once you are done come back here and execute the next cell to see what your randomly initialized network predicts. Try running the cell multiple times to see what happens with different weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = Perceptron()\n",
    "p.initialize_weights()\n",
    "colors = np.array(tuple(\"tab:blue\" if p.predict(x, vectorized=True) == -1 else \"tab:orange\" for x in data[[\"x\", \"y\"]].to_numpy()))\n",
    "plt.scatter(data[\"x\"].array, data[\"y\"].array, color = colors)\n",
    "plt.title(f\"{p.weights=}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Decision Boundary\n",
    "As you can see, the Perceptron appears to make decision based on whether a sample is positioned on one side or the other of a decision boundary. \n",
    "\n",
    ">**Task 4** Open [plotting.py](./plotting.py) and implement the function `plot_decision_boundary()`.\n",
    ">It takes as inputs a [`matplotlib.axis.Axis`](https://matplotlib.org/stable/api/axis_api.html#matplotlib.axis.Axis) object, as well as the weights and the bias of a Perceptron.\n",
    ">If you are new to matplotlib, read the [welcome guide](https://matplotlib.org/stable/users/explain/quick_start.html).\n",
    "When you are done, run the next cell to check that your code is correct. The plotted line should cleanly separate the points into two groups with different colors.\n",
    "Make sure that plotting the line does not change the scaling of the figure, i.e. do not plot outside the current `x_lim` and `y_lim` of the plot.\n",
    "\n",
    "*Hint:* This exercise might be harder than it appears. If you plot the line manually, for example by computing the formula of the line and writing it in Python, you will have to handle several different cases (computers do not like division by zero). You can use the matplotlib [`contour`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html) method, which handles these cases for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "p = Perceptron()\n",
    "p.initialize_weights()\n",
    "colors = np.array(tuple(\"tab:blue\" if p.predict(x, vectorized=True) == -1 else \"tab:orange\" for x in data[[\"x\", \"y\"]].to_numpy()))\n",
    "ax.scatter(data[\"x\"].array, data[\"y\"].array, color = colors)\n",
    "plot_decision_boundary(ax, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing data and algorithms is part of the the ML workflow. It helps you understand algorithms and share your results, and is therefore an extremely useful tool to have.\n",
    "Run the next cell to use your plotting code to gain a better understanding of the weigths and bias in a perceptron. Feel free to run the cell several times and to change the three tuples `bias`, `w1` and `w2`\n",
    "\n",
    ">**Task 5** Discuss: Can you come up with a geometric interpretation of the weights and bias of the perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "fig, axs = plt.subplots(3, 3)\n",
    "#p = Perceptron()\n",
    "#p.initialize_weights()\n",
    "bias = (-0.5, 0, 0.5)\n",
    "w1 = (-1, -0.6666, -0.333)\n",
    "w2 = (-0.5, 0, 0.5)\n",
    "def predict_and_plot(ax, p, point_size):\n",
    "    colors = np.array(tuple(\"tab:blue\" if p.predict(x, vectorized=False) == -1 else \"tab:orange\" for x in data[[\"x\", \"y\"]].to_numpy()))\n",
    "    ax.scatter(data[\"x\"].array, data[\"y\"].array, color = colors, s=point_size)\n",
    "    plot_decision_boundary(ax, p)\n",
    "for ax, bias in zip(axs[0], bias):\n",
    "    bias_old = p.bias\n",
    "    p.bias = bias\n",
    "    predict_and_plot(ax, p, 1)\n",
    "    p.bias = bias_old\n",
    "for ax, w in zip(axs[1], w1):\n",
    "    w_old = p.weights[0]\n",
    "    p.weights[0] = w\n",
    "    predict_and_plot(ax, p, 1)\n",
    "    p.weights[0] = w_old\n",
    "for ax, w in zip(axs[2], w2):\n",
    "    w_old = p.weights[1]\n",
    "    p.weights[1] = w\n",
    "    predict_and_plot(ax, p, 1)\n",
    "    p.weights[1] = w_old\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Perceptron\n",
    "\n",
    "Now that we can use our Perceptron to predict labels, we would it to make the correct predictions.\n",
    "For this we need to adapt (i.e. learn) its parameters (weights & bias).\n",
    "\n",
    ">**Task 6** Open [`perceptron.py`](./perceptron.py) and implement the `update_step()` method, the `train_epoch()` and the `fit()` methods.\n",
    "\n",
    "This next task illustrates the core of machine learning, and why it is called this way. First, we define a mathematical object, or model, (the Perceptron in our case) that can solve a problem (in our case: classify data). Then, we find a rule that updates the parameters (weights & bias in our case) of that mathematical object such that it solves the problem correctly. The solution, i.e. the correct parameters, are not given by the programmer but found in a mathematical optimization process. When such models become complex with hundreds, thousands or even billions of parameters (as is the case in modern large language models), it becomes hard to understand what exactly these models base their decisions on. This is the reason why machine learning models are often described as \"black boxes\", and why you can read articles claiming that scientists do not understand their machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron()\n",
    "p.initialize_weights()\n",
    "accuracies = fit(p, data[[\"x\", \"y\"]].to_numpy(), data[\"label\"].array, eta=0.0001, max_epochs=1000, stop_accuracy=0.999)\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "predict_and_plot(axs[0], p, 10)\n",
    "axs[1].plot(accuracies)\n",
    "plt.show()\n",
    "print(f\"Final accuracy: {accuracies[-1]}\\n{p.weights=}\\n{p.bias=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "One missing aspect of the ML workflow that you have not done yet in this notebook is hyperparameter optimization.\n",
    "Most ML algorithms have many hyperparameters (like $\\eta$ in the perceptron's case) with intricate effects.\n",
    "Modifying these hyperparameters can affect the learning speed and the performance of an algorithm.\n",
    "The process of finding the optimal hyperparameters is usually done automatically in a process called hyperparameter optimization.\n",
    "In the case of the Perceptron, we only have one parameter. We can therefore avoid the complexity of automatic hyperparameter search and find a good parameter manually.\n",
    "In the previous example we have set $\\eta$ to $0.0001$.\n",
    "\n",
    "> **Task 7** Find a better value for $\\eta$. The perceptron should be able to classify with 100 percent accuracy after a few epochs. You can modify the value of $\\eta$ by modifying `ETA` in [`perceptron.py`](./perceptron.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron()\n",
    "p.initialize_weights()\n",
    "accuracies = fit(p, data[[\"x\", \"y\"]].to_numpy(), data[\"label\"].array, max_epochs=1000, stop_accuracy=0.999)\n",
    "plt.plot(accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.show()\n",
    "print(f\"{accuracies=}, {p.weights=}, {p.bias=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task 8** Share the weights your algorithm found among each other and compare your results. Did you all find the same weights? Are some weights better than other ones and are there unique optimal weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Simple Dataset\n",
    "In the following we define a very simple minimal dataset consisting of only four data points and train a perceptron on it.\n",
    "\n",
    ">**Task 9** Discuss: Which logical operator is behind this dataset? Does the Perceptron algorithm work for this dataset? What property of this dataset causes the Perceptron algorithm to fail?\n",
    "\n",
    "Hint: Plotting the dataset & the decision boundary will help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "labels = np.array([-1, 1, 1, -1])\n",
    "p = Perceptron()\n",
    "p.initialize_weights()\n",
    "accuracies = fit(p, data, labels, 1000, 0.999)\n",
    "plt.plot(accuracies)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
