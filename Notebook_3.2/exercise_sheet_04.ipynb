{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e913afbc",
   "metadata": {},
   "source": [
    "# Notebook 4: Logistic Regression\n",
    "\n",
    "### Machine Learning Basic Module\n",
    "Florian Walter, Tobias JÃ¼lg, Pierre Krack\n",
    "\n",
    "Please obey the following implementation and submission guidelines.\n",
    "\n",
    "## General Information About Implementation Assignments\n",
    "We will use the Jupyter Notebook for our implementation exercises. The task description will be provided in the notebook. The code is also run in the notebook. However, the implementation itself is done in additional files which are imported in the notebook. Please do not provide any implementation that you want to be considered for correction in this notebook, but only in python files in the marked positions. A content of a python file could for example look similar as shown below:\n",
    "```python\n",
    "def f():\n",
    "    ########################################################################\n",
    "    # YOUR CODE\n",
    "    # TODO: Implement this function\n",
    "    ########################################################################\n",
    "    pass\n",
    "    ########################################################################\n",
    "    # END OF YOUR CODE\n",
    "    ########################################################################\n",
    "```\n",
    "To complete the exercise, remove the `pass` command and only use space inside the `YOUR CODE` block to provide a solution. Other lines within the file may not be changed in order to deliver a valid submission.\n",
    "\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b1c80e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "width = 9.5\n",
    "plt.rcParams['figure.figsize'] = [width, width / 1.618] \n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "UTNRED = \"#f5735f\"\n",
    "UTNBLUE = \"#0087dc\"\n",
    "mpl.rcParams['path.simplify'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b9eab7",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959f641",
   "metadata": {},
   "source": [
    "The discrimination function of logistic regression classification looks very similar to the Perceptron that we looked at in our first exercise:\n",
    "\n",
    "$$\\sigma(w^Tx + b)$$\n",
    "\n",
    "with the features $x\\in\\mathbb{R}^d$ with dimensionality $d\\in\\mathbb{N}$, the weights $w\\in\\mathbb{R}^d$, the bias $b\\in\\mathbb{R}$ and\n",
    "\n",
    "$$\\sigma(a) = \\frac{1}{1+e^{-a}}$$\n",
    "\n",
    "The *Sigmoid* function $\\sigma: \\mathbb{R}\\rightarrow (0, 1)$ is shown in the plot in the cell below. The difference to the Perceptron is the activation function: Instead of a step function from -1 to 1, the sigmoid is a smooth transition from 0 to 1, which can also be interpreted as a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c099210",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "values = np.arange(-10, 10, 0.1)\n",
    "plt.plot(values, sigmoid(values), color=UTNBLUE)\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('$\\sigma(a)$')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12638e8",
   "metadata": {},
   "source": [
    "As you will see later, the sigmoid function is the special case of the softmax function which is usually used as the last layer in deep neural networks for classification problems.\n",
    "Thus, from a deep learning perspective, this model can already be seen as a neural network with a single hidden layer.\n",
    "However, in this exercise we want to look at this from the probabilistic perspective and figure out why the sigmoid function (also called the logistic function) makes sense from coming from probability theory.\n",
    "\n",
    "Let's assume that we have two classes K = {0, 1}. $y$ denotes the class and $x$ is the feature variable.\n",
    "\n",
    "> **Task 1** Show that the posterior probability for class 1 given features $x$, ($p(y=1|x)$) is equal to $\\sigma(a)$ with\n",
    "> \n",
    "> $$a = \\ln \\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)}$$\n",
    "> \n",
    "> Hint: Bishop 4.2 might help you to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1350a19",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78b615",
   "metadata": {},
   "source": [
    "Now, given that $p(y=1|x) = \\sigma(a)$ with $a$ from above, we would like to show $a$ can be represented as $w^Tx + b$.\n",
    "We assume Gaussian likelihood $\\mathcal{N}(x|\\mu_c, \\Sigma)$ with class means $\\mu_c$ and the same covariance matrix $\\Sigma$ for all classes. We do not fix our priors and denote them with $p(y=c)$. Like above, assume that we only have two classes $c\\in\\{0, 1\\}$.\n",
    "\n",
    "$$p(x|y=c) = \\mathcal{N}(x|\\mu_c, \\Sigma) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp\\left\\{ -\\frac{1}{2} (x - \\mu_c)^T \\Sigma^{-1} (x - \\mu_c) \\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "> **Task 2** Given that assumption, show that the following equality holds: \n",
    ">\n",
    "> $$p(y=1|x) = \\sigma(w^Tx + b)$$\n",
    ">\n",
    "> with\n",
    "> \n",
    "> $$\n",
    " \\begin{aligned}\n",
    " w &= \\Sigma^{-1} (\\mu_1 - \\mu_0)\\\\\n",
    " b &= -\\frac{1}{2} \\mu_1^T \\Sigma^{-1} \\mu_1 + \\frac{1}{2} \\mu_0^T \\Sigma^{-1} \\mu_0 + \\ln \\frac{p(y=1)}{p(y=0)}\n",
    " \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce16d5a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$a = \\ln \\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)} = ...$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e77e4c",
   "metadata": {},
   "source": [
    "Since both $w$ and $b$ only include parameters which are depended on the data, we can also directly learn $w$ and $b$.\n",
    "With the bias rewriting trick we can include the bias in the weight and simply to $\\sigma(w^Tx)$ where we only have to learn $w$.\n",
    "\n",
    "To train the model an a 2-class dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^{N}$ ($x_i\\in \\mathbb{R}^d$, $y_i\\in \\{0, 1\\}$) we have to maximize the joint probability over the whole dataset:\n",
    "\n",
    "$$\n",
    "p(y|w) = \\prod_{i=1}^{N} \\sigma(w^Tx_i)^{y_i} (1- \\sigma(w^Tx_i))^{1-y_i}\n",
    "$$\n",
    "\n",
    "Using the negative log-likelihood we get an easy-to-differentiate error function which we can optimize. This function is called the binary cross-entropy as it expresses the entropy of the probability distribution:\n",
    "\n",
    "$$E(w) = -\\ln p(y|w) = -\\sum_{i=1}^{N}\\left( y_i\\ln(\\sigma(w^Tx_i)) + ({1-y_i})\\ln(1- \\sigma(w^Tx_i))\\right)$$\n",
    "\n",
    "\n",
    "> **Task 3** Assume that $\\mathcal{D}$ is a linearly separable dataset for 2-class classification, i.e. there exists\n",
    "> a vector w such that $\\sigma(w^Tx)$ separates the classes. Show that magnitute of the maximum likelihood parameter $w$\n",
    "> of a logistic regression model approaches infinity ($||w||\\rightarrow \\infty$). Assume that $w$ contains the bias term.\n",
    ">\n",
    "> Hint: You can use the fact that $\\sigma(a) \\in (0, 1)$ for all $a\\in\\mathbb{R}$.\n",
    "> \n",
    "> How can we modify the training process to prefer a $w$ of finite magnitude?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2806843",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99505557",
   "metadata": {},
   "source": [
    "## Properties of the Sigmoid Function\n",
    "\n",
    "### Derivative of the Sigmoid Function\n",
    "\n",
    "> **Task 4** Show that the derivative of the Sigmoid function $\\sigma(a) = \\frac{1}{1+e^{-a}}$ can be written as\n",
    ">\n",
    "> $$\\frac{\\partial\\sigma(a)}{\\partial a} = \\sigma(a)(1-\\sigma(a))$$\n",
    "\n",
    "Give your solution as tex code using in the cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c81e1f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\frac{\\partial \\sigma(a)}{\\partial a} = ... $$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f1ea95",
   "metadata": {},
   "source": [
    "### Deepening: Symmetrical Properties of the Sigmoid Function (optional)\n",
    "\n",
    "Show that the following equality holds for the Sigmoid function:\n",
    "\n",
    "$$\\sigma(-a) = 1 - \\sigma(a)$$\n",
    "\n",
    "Why does this equation hold, what is the intuition behind it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b8a5057c7e2cecd197d95ba04c7f4e3876d0ce5d765d787bef6dc282ede963d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
