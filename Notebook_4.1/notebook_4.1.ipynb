{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e913afbc",
   "metadata": {},
   "source": [
    "# Notebook 4.1: Support Vector Machines\n",
    "\n",
    "### Machine Learning Basic Module\n",
    "Florian Walter, Tobias JÃ¼lg, Pierre Krack\n",
    "\n",
    "Please obey the following implementation and submission guidelines.\n",
    "\n",
    "## General Information About Implementation Assignments\n",
    "We will use the Jupyter Notebook for our implementation exercises. The task description will be provided in the notebook. The code is also run in the notebook. However, the implementation itself is done in additional files which are imported in the notebook. Please do not provide any implementation that you want to be considered for correction in this notebook, but only in Python files in the marked positions. A content of a python file could for example look similar as shown below:\n",
    "```python\n",
    "def f():\n",
    "    ########################################################################\n",
    "    # YOUR CODE\n",
    "    # TODO: Implement this function\n",
    "    ########################################################################\n",
    "    pass\n",
    "    ########################################################################\n",
    "    # END OF YOUR CODE\n",
    "    ########################################################################\n",
    "```\n",
    "To complete the exercise, remove the `pass` command and only use space inside the `YOUR CODE` block to provide a solution. Other lines within the file may not be changed in order to deliver a valid submission.\n",
    "\n",
    "## General Information About Theory Assignments\n",
    "This Jupyter Notebook also includes one or more theory assignments. The theory assignments have to be solved ina PDF file.\n",
    "\n",
    "You can either typeset your solution in $\\LaTeX$/Word or hand-in a digital written or scanned solution.\n",
    "\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1c80e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from svm import solve_dual_svm, compute_weights_and_bias, plot_data_with_hyperplane_and_support_vectors, ALPHA_TOL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e4f52",
   "metadata": {},
   "source": [
    "## Binary SVM Classifier (hard margin)\n",
    "In this exercise we want to implement our own binary SVM classifier.\n",
    "We will start by creating a synthetic dataset. Run the cell below to create and visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200  # number of samples\n",
    "D = 2  # number of dimensions\n",
    "C = 2  # number of classes\n",
    "seed = 1234  # for reproducible experiments\n",
    "\n",
    "# alpha_tol = 1e-4 # threshold for choosing support vectors\n",
    "\n",
    "X, y = make_blobs(n_samples=N, n_features=D, centers=C, random_state=seed)\n",
    "y[y == 0] = -1  # it is more convenient to have {-1, 1} as class labels (instead of {0, 1})\n",
    "y = y.astype(float)\n",
    "plt.figure(figsize=[10, 8])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85009de",
   "metadata": {},
   "source": [
    "In this week's learning unit you should have learned that the SVM dual problem can be formulated as a Quadratic programming (QP) problem by solving the Lagrange dual problem. For a dataset with $N$ data points\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N\n",
    "$$\n",
    "\n",
    "with features $\\mathbf{x}_i \\in \\mathbb{R}^D$ and binary labels $y_i \\in \\{-1, 1\\}$,\n",
    "the SVM's dual problem is given as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{\\mathbf{\\alpha}} &\\quad g(\\mathbf{\\alpha}) = \\max_{\\mathbf{\\alpha}} \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j = \\\\\n",
    "\\text{subj.} &\\quad \\sum_{i=1}^N \\alpha_i y_i = 0 \\\\\n",
    "& \\alpha_i \\ge 0 \\quad \\text{for} \\quad i = 1, \\dots, N\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "where $\\mathbf{\\alpha}\\in\\mathbb{R}_{\\ge 0}^N$ is the vector of Lagrange multipliers.\n",
    "\n",
    "> **Task 1** Show that the dual function $g(\\alpha)$ can be written as\n",
    "> \n",
    "> $$g(\\alpha) = \\frac{1}{2}\\alpha^T\\mathbf{Q}\\alpha + \\alpha^T \\mathbf{1}_N$$\n",
    "> \n",
    "> by providing the matrix $\\mathbf{Q}$ using the vector of labels $\\mathbf{y}\\in\\mathbb{R}^N$ and feature matrix $\\mathbf{X}\\in\\mathbb{R}^{N\\times D}$ such that terms are equal. Denote the element-wise product between two matrices (in case you want to use it) by $\\odot$ (also known as Hadamard product or Schur product)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54a6b1",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ef3cf",
   "metadata": {},
   "source": [
    "We will solve the dual problem using a QP solver from the [`CVXOPT` library](https://cvxopt.org/) (a python library for convex optimization).\n",
    "\n",
    "The `CVXOPT` solver expects its QP problem to be in the following form:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{\\mathbf{x}}{\\min}\\quad\n",
    "    &\\frac{1}{2}\\mathbf{x}^T \\mathbf{P} \\mathbf{x} + \\mathbf{q}^T \\mathbf{x} \\\\\n",
    "\\text{subj.}\\quad\n",
    "    &\\mathbf{G}\\mathbf{x} \\le \\mathbf{h}\\\\\n",
    "    &\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> **Task 2** Formulate the SVM dual problems as a QP of this form and solve it using `CVXOPT`, i.e. specify the matrices $\\mathbf{P}, \\mathbf{G}, \\mathbf{A}$ and vectors $\\mathbf{q}, \\mathbf{h}, \\mathbf{b}$. Implement the `solve_dual_svm()` function in the `svm.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1e95b",
   "metadata": {},
   "source": [
    "Having obtained the optimal $\\alpha^*$ using our QP solver, we can compute the parameters defining the separating hyperplane.\n",
    "\n",
    "Recall, that from the optimality condition, the weights $w$ are a linear combination of the training samples,\n",
    "$$\n",
    "w = \\sum_{i=1}^{N} \\alpha_i^* y_i x_i\n",
    "$$\n",
    "\n",
    "From the complementary slackness condition $\\alpha_i^* f_i(\\theta^*) = 0$ we can easily recover the bias.\n",
    "\n",
    "When we take any vector $x_i$ for which $\\alpha_i \\neq 0$. The corresponding constraint $f_i(w, b)$ must be zero and thus we have\n",
    "$$\n",
    "w^T x_i + b = y_i.\n",
    "$$\n",
    "\n",
    "Solving this for $b$ yields the bias\n",
    "$$\n",
    "b = y_i - w^T x_i\n",
    "$$\n",
    "\n",
    "> **Task 3** Given this information, implement the `compute_weights_and_bias()` function in the `svm.py` file.\n",
    "\n",
    "\n",
    "Run the cell below to visualize the decision boundary and the support vectors. The code should work without any errors if you have implemented the functions correctly. Check the printed output of the cell with the reference solution given below. If your implementation is correct, the output should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd02211",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = solve_dual_svm(X, y)\n",
    "w, b = compute_weights_and_bias(alpha, X, y)\n",
    "print(\"w =\", w)\n",
    "print(\"b =\", b)\n",
    "print(\"support vectors:\", np.arange(len(alpha))[alpha > ALPHA_TOL])\n",
    "plot_data_with_hyperplane_and_support_vectors(X, y, alpha, w, b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229aec64",
   "metadata": {},
   "source": [
    "The reference solution is\n",
    "\n",
    "    w = array([0.73935606 0.41780426])\n",
    "    \n",
    "    b = 0.91993713\n",
    "\n",
    "Indices of the support vectors are\n",
    "    \n",
    "    [ 78 134 158 182]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aea011",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "Consider the following two plots.\n",
    "On the left is the original data that we want to separate linearly.\n",
    "On the right is the augmented data which comes from the feature mapping function $\\phi((x_0, x_1)) = (x_0, x_1, x_0^2, x_1^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971fc5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=seed)\n",
    "data = rng.uniform(low=-1, high=1, size=(2, 100))\n",
    "index_class_1 = np.logical_and(np.abs(data[0]) < 0.4, np.abs(data[1]) < 0.4)\n",
    "class_1 = data[:, index_class_1]\n",
    "class_2 = data[:, np.logical_not(index_class_1)]\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.scatter(class_1[0], class_1[1])\n",
    "ax.scatter(class_2[0], class_2[1])\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "def phi(x):\n",
    "    return np.array([x[0], x[1], x[0]**2 + x[1]**2])\n",
    "class_1 = phi(class_1)\n",
    "class_2 = phi(class_2)\n",
    "ax.scatter(class_1[0], class_1[1], class_1[2])\n",
    "ax.scatter(class_2[0], class_2[0], class_2[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61e38c",
   "metadata": {},
   "source": [
    "This mapping allows us to make the data linearly separable in the feature space.\n",
    "> **Task 4** Find a kernel function which implicitly uses the feature mapping $\\phi((x_0, x_1)) = (x_0, x_1, x_0^2 + x_1^2)^T$ (plotted above).\n",
    "$$\n",
    "\\newcommand{\\defeq}{\\stackrel{\\text{def}}{=}}\n",
    "\\newcommand{\\x}{\\textrm{x}}\n",
    "\\newcommand{\\xprime}{\\x^\\prime}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8cfc62",
   "metadata": {},
   "source": [
    "In this case, finding a feature mapping that works is quite simple and we can even plot our mapping to verify if it works.\n",
    "When the data becomes more complex choosing an appropriate feature mapping becomes considerably harder.\n",
    "Using kernels we can circumvent this problem.\n",
    "\n",
    "A *kernel function* $k$ is defined as $k: \\mathbb{R}^N\\times \\mathbb{R}^N \\rightarrow \\mathbb{R}$\n",
    "Some problems can be reformulated in terms of a kernel function (c.f. Bishop 6.1).\n",
    "$$k(x_i, x_j) \\defeq \\phi(x_i)^T\\phi(x_j) $$\n",
    "It turns out that in those cases, we can simply choose any kernel, some of which implicitly use an \"infinite-dimensional\" feature space.\n",
    "\n",
    "> **Task 5** Find out how this is possible by calculating the feature transformation $\\phi(x)$ corresponding to the kernel: \n",
    "> $$k(x_1, x_2) = \\frac{1}{1-x_1x_2}, \\text{ with } x_1, x_2 \\in (0, 1)$$\n",
    "Note: $x_1$ and $x_2$ are simply 1d vectors / scalars.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c4580",
   "metadata": {},
   "source": [
    "To come up with new kernels, it is possible to combine kernels in various ways (cf. Bishop 6.2 \"Techniques for Constructing New Kernels\").\n",
    "\n",
    "> **Task 6** Show that for $N\\in\\mathbb{N}$ and $a_i \\ge 0$ for $i\\in\\{0, ..., N\\}$ the following function $k$ is a valid kernel.\n",
    ">\n",
    "> $$k(x_1, x_2) = \\sum_{i=1}^{N}a_i\\left(x_1^T x_2\\right)^i + a_0, \\text{ with } x_1, x_2 \\in\\mathbb{R}^d$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf031e2b",
   "metadata": {},
   "source": [
    "A commonly used kernel is the Gaussian kernel:\n",
    "$$k(\\textrm{x}, \\textrm{x}^\\prime) = \\textrm{exp}\\left(- \\frac{||\\textrm{x} - \\textrm{x}^\\prime||^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "> **Task 7** Show that the gaussian kernel can be expressed as the inner product of an infinite-dimensional feature vector.\n",
    "> \n",
    "> *Hints*\n",
    "> * Start by transforming the kernel into a product of three exponentials \n",
    "$$k(\\x, \\xprime) = \\exp(\\ldots)\\exp(\\ldots)\\exp(\\ldots)$$\n",
    "> * Then consider the univariate case $k(x, x^\\prime)$ where $x$ and $x^\\prime$ are scalars. Use the definition of the exponential function to expand the middle exponential into an infinite sum.\n",
    "Only consider the middle exponential term for now.\n",
    "I.e. transform $k(x, x^\\prime) = \\exp\\left(\\frac{x x^\\prime}{\\sigma^2}\\right)$ into an infinite sum, then into a inner product of vectors $\\phi(x)^T\\phi(x^\\prime)$\n",
    "> * Finally, use your derived $\\phi(x)$ and the kernel techniques in Bishop 6.2 (\"Techniques for Constructing New Kernels\") to show that the gaussian kernel can be expressed as an inner product of an infinite-dimensional feature vector.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b8a5057c7e2cecd197d95ba04c7f4e3876d0ce5d765d787bef6dc282ede963d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
