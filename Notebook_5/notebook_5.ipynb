{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e913afbc",
   "metadata": {},
   "source": [
    "# Notebook 5: Artificial Neural Networks and Deep Learning\n",
    "\n",
    "### Machine Learning Basic Module\n",
    "Florian Walter, Tobias Jülg, Pierre Krack\n",
    "\n",
    "Please obey the following implementation guidelines.\n",
    "\n",
    "## General Information About Implementation Assignments\n",
    "We will use the Jupyter Notebook for our implementation exercises. The task description will be provided in the notebook. The code is also run in the notebook. However, the implementation itself is done in additional files which are imported in the notebook. Please do not provide any implementation that you want to be considered for correction in this notebook, but only in Python files in the marked positions. A content of a python file could for example look similar as shown below:\n",
    "```python\n",
    "def f():\n",
    "    ########################################################################\n",
    "    # YOUR CODE\n",
    "    # TODO: Implement this function\n",
    "    ########################################################################\n",
    "    pass\n",
    "    ########################################################################\n",
    "    # END OF YOUR CODE\n",
    "    ########################################################################\n",
    "```\n",
    "To complete the notebook, remove the `pass` command and only use space inside the `YOUR CODE` block to provide a solution. Other lines within the file may not be changed in order to deliver a valid submission.\n",
    "\n",
    "## General Information About Theory Assignments\n",
    "This Jupyter Notebook also includes one or more theory assignments. The theory assignments have to be solved and submitted in a PDF file which has to be named **theory.pdf** and has to include the solutions of all assignments.\n",
    "\n",
    "You can either typeset your solution in $\\LaTeX$/Word or hand-in a digital written or scanned solution. \n",
    "Please make sure to always submit a **PDF file**.  If you decide to submit a handwritten solution please create it in a way that it is possible for externals to read. We will not consider solutions which we cannot read. Thus, we recommend to typeset your solution.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1c80e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import NamedTuple\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from micrograd.autograd import add, mul, backward, tanh, neuron\n",
    "from micrograd.graph import topological_sort, draw_dot, DataNode, Graph, LABELS\n",
    "from macrograd.dataset import SimpleDataset\n",
    "from macrograd.layers import Linear, ReLULayer, Sigmoid, BinaryCrossEntropyLoss, CategoricalCrossEntropyLoss\n",
    "from macrograd.optimizer import SGDOptimizer\n",
    "from macrograd.trainer import BinaryTrainer, CategoricalTrainer\n",
    "from macrograd.utils import seeding\n",
    "\n",
    "seeding(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb0f21",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\diff}[1]{\\frac{\\text{d}}{\\text{d}#1}}$$\n",
    "\n",
    "# Autograd from Scratch\n",
    "\n",
    "In this assignment you will implement the method upon which most of modern machine learning builds upon: automatic differentiation.\n",
    "We will keep it simple and focus on the univariate case.\n",
    "The multivariate case is similar, but more complex to implement—engineers from Meta did that job for you and called it PyTorch.\n",
    "\n",
    "## Automatic differentiation and backpropagation\n",
    "\n",
    "As the name implies, the goal of automatic differentiation (AD) is to compute derivatives algorithmically.\n",
    "\n",
    "Multiple methods co-exist in the broader field of computer science: source code transformations, automatic differentiation with dual numbers, forward and reverse mode accumulation.\n",
    "\n",
    "The machine learning community mostly uses the algorithm you will implement: a special case of reverse mode accumulation where the function being differentiated has a scalar (one-dimensional) output.\n",
    "\n",
    "What is this function that always has a scalar output? Why is automatic differentiation different if we have a function where the output is multi-dimensional? Discuss with each other and/or ask us if you are unsure!\n",
    "\n",
    "In machine learning, we are interested in how much our loss changes *for a specific input*.\n",
    "Therefore, we only need to evaluate derivatives at specific inputs and never actually write down the full derivative function.\n",
    "\n",
    "### Reverse and forward mode AD\n",
    "Assume you have three functions $f$, $g$ and $h$ that are chained together such that $y = f(g(h(x)))$ and let $a$ and $b$ be intermediate results: $h(x) = a$ and $g(h(x)) = g(a) = b$.\n",
    "Reverse mode AD (so also backpropagation), finds the derivative of $y$ with respect to $x$ by first computing $$\\diff{b} f(b)\\text{ i.e. how much does $y$ change when $b$ changes, then}$$\n",
    "$$\\diff{a} f(g(a)) = \\diff{b} f(b) \\cdot \\diff{a} g(a)\\text{ i.e. how much does $y$ change when $a$ changes, and finally}$$\n",
    "$$\\diff{x} f(g(h(x))) = \\diff{b} f(b) \\cdot \\diff{a} g(a) \\cdot \\diff{x} h(x) \\text{ i.e. how much does $y$ change when $x$ changes.}$$\n",
    "Forward mode AD goes the other way around.\n",
    "It starts with $\\diff{x} h(x)$ then computes $\\diff{x} g(h(x))$ etc.\n",
    "\n",
    "Does it make sense why it is called *reverse* mode AD and *back*propagation?\n",
    "\n",
    "## The compute graph\n",
    "\n",
    "The very first step is to know what the functions $f_i$ are.\n",
    "We achieve this by defining up front which functions are available and restricting the user to use those functions.\n",
    "Whenever these functions are used, we build the \"computational graph\". Let us start with some definitions.\n",
    "\n",
    "Each node in our graph will represent either a value with a gradient, e.g. the intermediate value $b = g(h(x))$ with the gradient $\\diff{b} f(g(h(x)))$,\n",
    "```python\n",
    "@dataclass(eq=False)\n",
    "class DataNode:\n",
    "    data: float\n",
    "    gradient: float = 0\n",
    "```\n",
    "\n",
    "... or an operation (i.e. a function, e.g. $f$).\n",
    "\n",
    "```python\n",
    "class Op(Enum):\n",
    "    ADD = 0\n",
    "    MUL = 1\n",
    "\n",
    "@dataclass(eq=False)\n",
    "class OpNode:\n",
    "    op: Op\n",
    "```\n",
    "\n",
    "A graph is then, just as it is defined in every introductory computer science lecture, a tuple $G = (V, E)$ where $V$ is a set of nodes ($V$ stands for the mathematical term vertex) and $E$ is a set of edges (tuples $(u, v)$ with $u, v \\in V$).\n",
    "```python\n",
    "Node = DataNode | OpNode # A node is either a value with a gradient or an operation\n",
    "class Edge(NamedTuple):\n",
    "    u: Node\n",
    "    v: Node\n",
    "\n",
    "class Graph(NamedTuple):\n",
    "    V: set[Node]\n",
    "    E: set[tuple[Node]]\n",
    "```\n",
    "> **Task 1** Open [`micrograd/autograd.py`](micrograd/autograd.py) and implement the functions `add` and `mul`. `add` takes as input a Graph $G=(V, E)$ and two nodes $x$ and $y$, and returns a tuple $(G^\\prime, z)$, where $G^\\prime$ is the updated graph and z is the new resulting node with data equal to the sum of the data in $x$ and $y$. \n",
    "In the new graph, the nodes should include a new `OpNode` and a new `DataNode`, the edges should include new edges from $x$ and $y$ to the new `OpNode` and from the new `OpNode` to the new result `DataNode`.\n",
    "For inspiration, have a look at the implementation of the tanh activation function given in the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, w1, w2):\n",
    "    return mul(*add(Graph(set(), set()), DataNode(x), DataNode(w1)), DataNode(w2))\n",
    "g, out = f(1, .5, .7)\n",
    "draw_dot(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf57a97",
   "metadata": {},
   "source": [
    "Try to fill in the gradients manually.\n",
    "At each node $v$, the gradient should tell you how the output of the function changes if you change the value of $v$.\n",
    "Start at the last node (value 1.05) continue with the middle nodes (values 0.7 and 1.5) and finish with the first two nodes (values 0.5 and 1.0).\n",
    "\n",
    "For example, if you add $1$ to the node with data $0.7$, then the output of the function will be $1.5 \\cdot 1.7 = 2.55$.\n",
    "The change in the output is then $2.55 - 1.05 = 1.5$\n",
    "Be careful though, the technique \"add one and check how much the output changes\" only works with linear operations.\n",
    "\n",
    "You can also verify your result by using the definition of the derivative:\n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}x} f(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "Using the same example from before:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\text{d}}{\\text{d}x} 1.5x &= \\lim_{h \\to 0} \\frac{1.5(x+h) - 1.5x}{h} \\\\\n",
    "                                &= \\lim_{h \\to 0} \\frac{1.5h}{h} \\\\\n",
    "                                &= 1.5\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If you want a sufficient approximation to verify your results use a small $h$ e.g. $h = 0.0001$ and compute $\\frac{f(x) - f(x+h)}{h}$. \n",
    "\n",
    "The key to understanding backpropagation is in the first two nodes (with data 1.0 and 0.5)!\n",
    "Hopefully you have now understood how the backpropagation algorithm works.\n",
    "If not, make sure to ask us!\n",
    "\n",
    ">**Task 2** Use this insight to implement the functions `add_back` and `mul_back` which take two input data nodes $x$ and $y$ and an output node `out` and sets the gradients on $x$ and $y$.\n",
    "\n",
    "In the example above, we gave you the order in which you should compute these nodes.\n",
    "There are multiple node orderings that would work for computing gradients but not all of them.\n",
    "There is an ordering called \"topological ordering\" that that fulfills the conditions necessary for the backpropagation algorithm to work.\n",
    "Do a quick internet search on the topological ordering. \n",
    "Do you understand why this method of sorting nodes is appropriate?\n",
    "\n",
    "We implemented the `topological_sort` function in [`micrograd/graph.py`](micrograd/graph.py) for you, you can have a look at it if you want.\n",
    "\n",
    "We used the pseudo code on [wikipedia](https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm) to implement Kahn's algorithm which maps nicely to functional python code.\n",
    "\n",
    "The final step is now quite simple.\n",
    "\n",
    "> **Task 3** Implement the `backward` function. We implemented the skeleton for you. It iterates through the reversed topological order of the compute graph and identifies all triplets of (input nodes, operation nodes, output node). You just need to apply the correct backward function now.\n",
    ">\n",
    ">Hint:\n",
    ">* You can use the argument unpacking operator * to have a nice one-liner. e.g.\n",
    ">```python\n",
    "f(*range(3)) == f(0,1,2)\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ca0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward(g)\n",
    "draw_dot(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816b3e6",
   "metadata": {},
   "source": [
    "Here is a simple neuron using our autograd engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7063782",
   "metadata": {},
   "outputs": [],
   "source": [
    "g, out = neuron(Graph(set(), set()), *(DataNode((i+1)/10) for i in range(5)))\n",
    "backward(g)\n",
    "draw_dot(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b383e8",
   "metadata": {},
   "source": [
    "Now here is the same neuron with different input values. Why are the gradients all zero? Can you name this phenomenon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g, out = neuron(Graph(set(), set()), *(DataNode(i+1) for i in range(5)))\n",
    "backward(g)\n",
    "draw_dot(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f254b",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "In this exercise we will introduce `PyTorch`, one of the most commonly used frameworks for training deep neural networks as it provides a simple pythonic way of defining network models. Its main components include:\n",
    "- **Pytorch Tensor** Data container similar to `np.array` with the difference that it can be moved to a GPU to execute operations there.\n",
    "- **Autograd Engine** Automatic differentiation with a dynamic compute graph similar to the implementation shown in class but implemented in C++ for fast execution. Autograd can also be extended with new functions as we will see later on.\n",
    "- **`torch.nn.Module`** Base class to define neural networks in a composition-based fashion, e.g. a layer can also be seen as a neural network and is, thus, also a `nn.Module`. Many layers such as the linear layer (`nn.Linear`) or loss functions such as the categorical cross entropy loss (`nn.CrossEntropyLoss`) are already implemented in `nn` as subclassed `nn.Module`s.\n",
    "- **`torch.utils.data.Dataset`** Base class to define your data.\n",
    "- **`torch.utils.data.Dataloader`** Loads the data from a dataset. Provides functionality to parallelize data loading. Shuffles and splits the data into batches.\n",
    "- **`torchvision.transforms.v2`** Although in its own package, these functions can be used for preprocessing such as normalizing data samples.\n",
    "- **`torch.optim`** Implementation of weight update algorithms also called optimizers.\n",
    "\n",
    "We will reimplement parts of PyTorch in order to better understand what is going on under the hood when you are using it in future projects.\n",
    "\n",
    "## Tensors and Autograd\n",
    "You can think of PyTorch simply as a NumPy-like library which can execute calculations on the GPU and provides automatic gradient calculation with a few other utilities commonly used when training neural networks. It has its own container type, the tensor, but provides functions to convert from and to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94979e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2.])\n",
    "t = torch.from_numpy(a)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db9ac2",
   "metadata": {},
   "source": [
    "Currently this tensor os on the CPU, as numpy only uses the CPU. If you have a GPU and the correct installation of PyTorch, you can move the tensor to your GPU and perform calculations over there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f48eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.device)\n",
    "# checking if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    t = t.to(\"cuda:0\")\n",
    "    print(t.device)\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "\n",
    "# moving back to CPU\n",
    "t = t.cpu()\n",
    "print(t.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97dcf0",
   "metadata": {},
   "source": [
    "By default, PyTorch does not track gradients for arbitrary tensors. You can tell PyTorch to track the gradient of a tensior by setting its `requires_grad` attribute to `True`.\n",
    "To calculate the gradients after the forward pass you need to call the `backward()` method on the root node of the computational graph (typically you call `backward()` on the loss).\n",
    "After the backward pass the gradients can be accessed with the `grad` attribute.\n",
    "Before calling `backward()` the gradients are `None`.\n",
    "\n",
    "In the example below we want to calculate the derivative $f(x) = x^3$ for $x=2$ which should be $\\frac{d f(x)}{x} = 3*x^2 \\Rightarrow 3*2*2 = 12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0716e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.requires_grad = True\n",
    "print(t.grad)\n",
    "\n",
    "# perform calculation\n",
    "y = t**3\n",
    "# you will see the leaf node of the computation graph as the power operation\n",
    "print(y)\n",
    "\n",
    "# gradient is still None\n",
    "print(t.grad)\n",
    "\n",
    "# backward pass\n",
    "y.backward()\n",
    "\n",
    "# gradient is now available\n",
    "print(t.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9abc10",
   "metadata": {},
   "source": [
    "By default, PyTorch tracks the gradients of each tensor which has `requires_grad` set to `True`.\n",
    "If you do not need gradients then you should deactivate it as the forward computation becomes more efficient.\n",
    "A typical use case for this is the validation pass.\n",
    "You can deactivate gradient tracking by using `with torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2.])\n",
    "t = torch.from_numpy(a)\n",
    "t.requires_grad = True\n",
    "with torch.no_grad():\n",
    "    # no gradient tracking in here, even if requires_grad is True\n",
    "    y = t**3\n",
    "\n",
    "# backward pass would fail because no computational graph has been created\n",
    "# y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6718f3f",
   "metadata": {},
   "source": [
    "We have seen in class that we can calculate the gradients of arbitrary complex functions composed of simple base functions such as plus, minus, multiplication, etc. by using the computational graph with the chain rule. This is also how PyTorch computes the gradients: It overloads functions such as the exp function shown above and tracks the computational graph.\n",
    "\n",
    "PyTorch already provides the implementation for many use cases. However, sometimes, you might have to implement your own function along with its gradient because you might want to use advanced mathematics which cannot be expressed by simple arithmetic or there exists a simple derivative which is much more efficient than using the computational graph. We will explore the second reason for an example later on in this exercise.\n",
    "\n",
    "## Differentiable functions in PyTorch\n",
    "Let's explore the API provided by PyTorch to define autograd functions.\n",
    "To add a differentiable function to PyTorch, we need need to inherit from `torch.autograd.Function` and implement static forward and backward functions.\n",
    "An example for the sigmoid function is given below:\n",
    "\n",
    "```python\n",
    "class Sigmoid(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logit: torch.Tensor):\n",
    "        # for numeric stability we use the tanh formula here\n",
    "        sigmoid = 1/2 * (1+torch.tanh(logit/2))\n",
    "        ctx.save_for_backward(sigmoid)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        (sigmoid,) = ctx.saved_tensors\n",
    "        return grad_output * sigmoid * (1 - sigmoid)\n",
    "```\n",
    "\n",
    "The first argument of both functions is the autograd context object which is provided by PyTorch.\n",
    "It can be used to store information about the forward pass to reuse it in the backward pass.\n",
    "For example, the derivative of the sigmoid function can be expressed in terms of the sigmoid function itself:\n",
    "$$\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1-\\sigma(x))$$\n",
    "We can therefore store the output of the sigmoid function with `ctx.save_for_backward` to reuse it in the backward pass with `ctx.saved_tensors`.\n",
    "\n",
    "The other arguments in the forward pass are the function's arguments (in case of the sigmoid only one).\n",
    "\n",
    "In the backward pass the second argument is the accumulated upstream gradient which we need to multiply with our own gradient according to the chain rule.\n",
    "The backward function needs to output the gradient with respect to each parameter.\n",
    "Since we only have one parameter in the example above, it only outputs one gradient.\n",
    "If you are not interested in gradients for potential other parameters you can simply return `None` as the gradient value.\n",
    "This would for example be the case for loss functions where the second argument are the ground truth values.\n",
    "\n",
    "# Your turn!\n",
    "All the functions that we are going to implement in the following sections have to accept batches as input data, meaning that the shape of the input data is `(N, D)` where `N` is the batch size and `D` the data dimension. You are **not** allowed to use the equivalent pytorch functions for the following implementations and should rather implement them using with the use of base functions.\n",
    "\n",
    "The first function we implement is the ReLU activation function (REctified Linear Unit), one of the most commonly used activation functions in deep learning due to its simplistic non-linearity.\n",
    "> **Task 4** Calculate the derivative of the ReLU function give as\n",
    "> $$\\begin{aligned}&\\text{ReLU}: \\mathbb{R} \\rightarrow \\mathbb{R_0^+} \\\\ &\\text{ReLU}(x) = \\max(x, 0)\\end{aligned}$$\n",
    "> You can use the unit tests [`macrograd/test_layers.py`](macrograd/test_layers.py) to check whether you implementations returns the correct gradients.\n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\text{ReLU}(x)}{\\partial x} = ...$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb5894",
   "metadata": {},
   "source": [
    "> **Task 5** Implement the `forward()` and `backward()` functions of the `ReLU` class in [`macrograd/layers.py`](layers.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3579cd7",
   "metadata": {},
   "source": [
    "In order to solve classification tasks with two classes we usually combine two layers for the loss which are the Sigmoid function (already implemented above) to create a probability from the output neuron and the binary cross entropy which calculates the negative entropy between the output of the sigmoid $\\hat{y}$ and the ground truth values $y$ that we would like to learn. We have seen the entropy already in the logistic regression assignment. It shows the log probably that the $\\hat{y}$ is from the probability distribution of $y$. We thus want to maximize it i.e. minimize its negative.\n",
    "\n",
    "> **Task 6** Calculate the partial derivative (w.r.t $\\hat{y}$) of the negative binary cross entropy given as\n",
    "> $$\\begin{aligned}&\\text{BCE}: (0, 1)\\times (0, 1) \\rightarrow \\mathbb{R_0^+} \\\\ &\\text{BCE}(\\hat{y}, y) = -\\left( y \\log \\hat{y} + (1-y) \\log (1-\\hat{y}) \\right)\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4685282d",
   "metadata": {},
   "source": [
    "\n",
    "$$\\frac{\\partial \\text{BCE}(\\hat{y}, y)}{\\partial \\hat{y}} = ...$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8edd09",
   "metadata": {},
   "source": [
    "> **Task 7** Implement the `forward()` and `backward()` functions of the `BCE` class in [`macrograd/layers.py`](layers.py). Note that this function takes two inputs: the ground labels $y$ and the output of the network $\\hat{y}$ which means that the backward function also needs to return two gradients $\\frac{\\partial \\text{BCE}(\\hat{y}, y)}{\\partial \\hat{y}}$ and $\\frac{\\partial \\text{BCE}(\\hat{y}, y)}{\\partial y}$, however the latter is irelevant for backprobagation and you can, thus, just return `None` for it. Also note that you are receiving a batch but want to output a single loss value. In this sheet we will use `mean` as our reduction method. Thus, the actual function becomes the following, which you have to also consider in your `backward` function:\n",
    "> $$\\begin{aligned}&\\text{BCE}_\\text{impl}: (0, 1)^{N\\times 1}\\times (0, 1)^{N\\times 1} \\rightarrow \\mathbb{R_0^+} \\\\ &\\text{BCE}_\\text{impl}(\\hat{y}, y) = -\\frac{1}{N} \\sum_{i=1}^{N}\\left( y_i \\log \\hat{y}_i + (1-y_i) \\log (1-\\hat{y}_i) \\right)\\end{aligned}$$\n",
    "> You can use the unit tests [`macrograd/test_layers.py`](macrograd/test_layers.py) to check whether you implementations returns the correct gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d447b",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\deriv}[2]{\\frac{\\partial #1}{\\partial #2}}$$\n",
    "Now, we want to go multi class also called categorical. Thus, instead of one output neuron, we will now have $C$ where $C$ is the number of classes in our dataset. Like before the output neurons can have arbitrary values and, thus, we need to constraint them in order to get a valid probability distribution (all values between zero and one and all values sum up to one). To do so we use the softmax function, which is a generalization of the sigmoid function. We denote the function also with $\\sigma$ and its $j$-th component is given as $\\sigma(x)_j$.\n",
    "\n",
    "$$\\begin{aligned}&\\sigma: \\mathbb{R}^C \\rightarrow (0, 1)^C \\\\ \\text{Softmax}(x)_j =\\ &\\sigma(x)_j = \\frac{\\exp(x_j)}{\\sum_{i=1}^{C} \\exp(x_i)}\\end{aligned}$$\n",
    "which is the output of the $j$-th component.\n",
    "Note, that this function takes in a vector and outputs a vector thus, its derivative is a Jacobian Matrix. For $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}^m$ the Jacobian Matrix is defined as\n",
    "$$J_f = \\begin{bmatrix}\n",
    "\\deriv{f_1}{x_1} & \\dots & \\deriv{f_1}{x_n}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\deriv{f_m}{x_1} & \\dots & \\deriv{f_m}{x_n}\\\\\n",
    "\\end{bmatrix}$$\n",
    " \n",
    "How would the jacobi matrix of the softmax function look like?\n",
    "Hint: The softmax is the generalization of the sigmoid function. Try to write the derivative of the softmax in terms of the softmax function itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6cc75",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "We look at two cases: Derivative with respect to $x_j$ and derivative with respect to $x_k$ where $i\\neq k$.\n",
    "\n",
    "Case 1:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\deriv{\\sigma(x)_j}{x_j} &= \\frac{(\\sum_{i=1}^{C}\\exp(x_i))\\exp(x_j) - \\exp(x_j)\\exp(x_j)}{(\\sum_{i=1}^{C}\\exp(x_i))^2}\\\\\n",
    "&= \\underbrace{\\frac{\\exp(x_j)}{\\sum_{i=1}^{C}\\exp(x_i)}}_{\\sigma(x)_j}\\cdot \\underbrace{\\frac{\\sum_{i=1}^{C}\\exp(x_i) - \\exp(x_j)}{\\sum_{i=1}^{C}\\exp(x_i)}}_{1-\\sigma(x)_j}\\\\\n",
    "&= \\sigma(x)_j(1-\\sigma(x)_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Case 2:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\deriv{\\sigma(x)_j}{x_k} &= \\frac{-\\exp(x_j)\\exp(x_k)}{(\\sum_{i=1}^{C}\\exp(x_i))^2}\\\\\n",
    "&= -\\underbrace{\\frac{\\exp(x_j)}{\\sum_{i=1}^{C}\\exp(x_i)}}_{\\sigma(x)_j} \\cdot \\underbrace{\\frac{\\exp(x_k)}{\\sum_{i=1}^{C}x_i}}_{\\sigma(x)_k}\\\\\n",
    "&= -\\sigma(x)_j\\sigma(x)_k\n",
    "\\end{aligned}\n",
    "$$\n",
    "We can then simplify this even further by combing both cases with the Kornecker delta $\\delta_{ij}$:\n",
    "$$\\delta_{ij} = \\begin{cases}1\\quad i=j\\\\0\\quad i\\neq j\\end{cases}$$\n",
    "For $i\\in[C]$\n",
    "$$\\deriv{\\sigma(x)_j}{x_i} = (\\delta_{ij}-\\sigma(x_j))\\sigma(x_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc753df2",
   "metadata": {},
   "source": [
    "Even though single layers, such as the softmax function above, have a jacobian matrix for its derivative, we can actually avoid dealing with jacobians in deep learning by the following trick: The final layer, our loss function, always maps the output vector to a single scalar value. So when calculating the gradient with respect to the input for a layer and including all following layers up to the loss function, the jacobian matrix will always resolve into a gradient vector. We will see this effect in the following task where we calculate the gradient for a function which combines the softmax function and the categorical cross entropy loss.\n",
    "\n",
    "The categorical cross entropy loss is a generalization to the binary cross entropy loss to arbitrary many classes. It is defined as follows:\n",
    "$$\n",
    "\\begin{aligned}&\\text{CCE}: (0, 1)^C\\times (0, 1)^C \\rightarrow \\mathbb{R_0^+} \\\\\n",
    "&\\text{CCE}(\\hat{y}, y) = -\\sum_{i=1}^{C}\\left( y_i \\log \\hat{y}_i\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "> **Task 8** Calculate the derivative of the function that combines softmax and categorical cross entropy into one layer. Simplify as much as possible.\n",
    "> $$\\begin{aligned}&\\text{CCE}(\\text{Softmax}(x), y) = -\\sum_{i=1}^{C}\\left( y_i \\log \\sigma(x)_i\\right)\\end{aligned}$$\n",
    "> Hint: Take a one-hot encoding for $y$ as given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f14f85",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\frac{\\partial }{\\partial x_j}  -\\sum_{k=1}^{C}\\left( y_k \\log \\frac{\\exp(x_k)}{\\sum_{i=1}^{C} \\exp(x_i)}\\right)  = ...$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8777d0fd",
   "metadata": {},
   "source": [
    "Having done the derivation (and simplified as much as possible), you should see the second reason given above for why you would want to implement your own backward function: In this case the gradient is much easier when we fuse the two layers and, thus, more efficient to compute than using standard backpropagation which would calculate the gradient for each layer (and in this case having to deal with the more complex gradient of the softmax function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa3757",
   "metadata": {},
   "source": [
    "> **Task 9** Implement the `forward()` and `backward()` functions of the `CrossEntropy` class in [`macrograd/layers.py`](layers.py). This layer combines softmax and categorical cross entropy. You can use your derivation from above for the backward function.\n",
    "> Use the log-sum-exp trick to avoid numerical instability. Think how you can reformulate the joined formula to get a log-sum-exp expression. Why can the softmax be problematic in terms of numerical stability? Answer with a comment in the python file.\n",
    "> Like in BCE, the function receives two inputs $y$ and $\\hat{y}$. Be reminded that you are receiving a batch but want to output a single loss value. Like before we will use `mean` as our reduction method. Thus, the actual function becomes the following, which you have to also consider in the gradient of your `backward()` function:\n",
    "> $$\\begin{aligned}&\\text{CrossEntropy}_\\text{impl}: \\mathbb{R}^{N\\times C}\\times (0, 1)^{N\\times 1} \\rightarrow \\mathbb{R_0^+} \\\\ &\\text{CrossEntropy}_\\text{impl}(x, y) = -\\frac{1}{N} \\sum_{i=1}^{N}\\sum_{k=1}^{C}\\left( y_k \\log \\frac{\\exp(x_k)}{\\sum_{i=1}^{C} \\exp(x_i)}\\right)\\end{aligned}$$\n",
    "> You can use the unit tests [`macrograd/test_layers.py`](macrograd/test_layers.py) to check whether you implementations returns the correct gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac394c61",
   "metadata": {},
   "source": [
    "The only missing layer that we need to build a small neural network is a linear layer (also called fully connected or dense layer) which is essentially a Perceptron.\n",
    "Since this is a layer with parameters, we are going to use PyTorch's autograd for the gradient calculation.\n",
    "`torch.nn.Parameter` is a special type of tensor meant for model parameters which already have the `requires_grad` attribute set to `True`.\n",
    "\n",
    "> **Task 10** Implement the `__init__()`, `reset_parameters` and `forward()` methods of the `Linear` class in [`macrograd/layers.py`](macrograd/layers.py).\n",
    "Use `torch.nn.Parameter` to create parameters for weights and bias to implement the following linear function:\n",
    "> $$\\text{Linear}(x) = w^T x + b$$\n",
    "> `reset_parameters` should initialize the parameters as follows: $b$ should be initialized constantly to zero and $w$ randomly with the standard distribution. (We will have a closer look a proper initialization in the next exercise)\n",
    ">\n",
    "> Hint: Again you should be careful to handle batches in the correct way. *Broadcasting* should help you to avoid for-loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68875a8d",
   "metadata": {},
   "source": [
    "With all the things that we have implemented so far we can perform a forward pass, perform a backward pass using autograd and observe the parameter's gradients. The only thing left to implement is the algorithm that updates the weights given the gradient, also called the optimizer. We will simply use Stochastic Gradient Descent (SGD). Recall, that SGD uses a random subset of the dataset called the (mini) batch and updates the weights according to the negative gradient direction of the loss function with respect to the weights, multiplied by some learning rate $\\alpha$:\n",
    "\n",
    "$$w' = w - \\alpha \\nabla_w L(y, x)$$\n",
    "\n",
    "> **Task 11** Implement the `__init__()`, `step()` and `zero_grad()` methods of the `SGDOptimizer` class in the [`macrograd/optimizer.py`](macrograd/optimizer.py) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354beb7",
   "metadata": {},
   "source": [
    "So now, we have all the ingredients to create and train a small neural network even with several layers. What is still missing is the data. We use the same data as in our first exercise. You will find it in `data.csv`. In PyTorch data are abstracted into `Dataset` classes which essentially implement a Python iterator returning the $i$-th data sample.\n",
    "> **Task 12** Implement the `__init__()`, `__getitem__()` and `__len__()` functions of the `SimpleDataset` class in [`macrograd/dataset.py`](macrograd/dataset.py).\n",
    "\n",
    "If you are done, let's load the data. The following cell should not output an error if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd54204",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = \"macrograd/data.csv\"\n",
    "data = SimpleDataset(d_path)\n",
    "\n",
    "# these checks should not fail if your implementation is correct\n",
    "assert len(data) == 200\n",
    "rnd_idx = random.randint(0, 199)\n",
    "assert isinstance(data[rnd_idx], tuple)\n",
    "assert isinstance(data[rnd_idx][0], torch.Tensor)\n",
    "assert isinstance(data[rnd_idx][1], torch.Tensor)\n",
    "assert data[rnd_idx][0].shape == torch.Size([2])\n",
    "assert data[rnd_idx][1].shape == torch.Size([1])\n",
    "assert data[rnd_idx][0].type() == 'torch.FloatTensor'\n",
    "assert data[rnd_idx][1].type() == 'torch.FloatTensor'\n",
    "assert {i[1].item() for i in data} == {0., 1.}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06218b1",
   "metadata": {},
   "source": [
    "You should have learned the problem of overfitting in this week. The problem arises when the network learnt the training data by heart, thus, performing really good on it, but on the otherhand fails to generalize and, thus, performs rather bad on unseen data. To detect and mitigate this effect, we usually split the data into three categories:\n",
    "- Training data: The data that we are training on.\n",
    "- Testing data: During training we use this subset to check how well the model is performing on unseen data to avoid overfitting.\n",
    "- Validation data: Through us, the researchers, the model might also be optimized for the validation data in a hyper optimization as we select choices where the model performs better on the validation data. Thus, we use the test data at the very end to create an unbiased statement of the model performance on unseen data.\n",
    "\n",
    "It is very important that the splits still belong to the same feature distribution as deep neural networks can only predict within this distribution (out-of-distribution prediction / extrapolation is not possible with neural networks). Thus, one usually gathers one dataset and splits these subsets apart. However, here comes the other part the one has to be careful about: Splitting is data dependent. The splits are not allowed to have any information about data from the other splits, i.e. they must be independent.\n",
    "For example in time series data, one sequence should always be only in one split and not distributed among splits.\n",
    "\n",
    "Since we do not have this issue here, we can simply split the data randomly with a split of 80/20 as we also won't use a test data set. `torch.utils.data.random_split` can be used to split a torch `Dataset` into subsets as shown in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = torch.utils.data.random_split(data, [int(len(data)*0.8), int(len(data)*0.2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155eafd5",
   "metadata": {},
   "source": [
    "With the current setting, we can only iterate over the dataset sample by sample. However, for training the neural network we would like to use mini-batches. Furthermore, these mini-batches should be uncorrelated to avoid that the neural network learns a correlation between the samples in the same batch. Thus, after each epoch we would like to create new random mini-batches.\n",
    "\n",
    "PyTorch provides the `torch.utils.data.DataLoader` class which does exactly this while using multi processing to fetch the data samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train, batch_size=32, shuffle=True)\n",
    "val_data = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e25870",
   "metadata": {},
   "source": [
    "Now we would like to use all our previous code to actually train a neural network on the data.\n",
    "Have a look at the `Trainer` class in [`macrograd/trainer.py`](macrograd/trainer.py). It is meant as an abstract class for training deep neural networks. Concrete implementations might be different e.g. binary classification vs categorical classification and, thus, have to subclass and implement the concrete training process by implementing the `train_epoch()` and `val_epoch()` functions. You do not have to change anything at the `Trainer` code. However, if you did not manage to do the optimizer task you can use pytorch's SGD optimizer in `configure_optimizer`.\n",
    "\n",
    "> **Task 13** Implement the `train_epoch()` and `val_epoch()` methods of the `BinaryTrainer` class in the [`macrograd/trainer.py`](macrograd/trainer.py) file.\n",
    "> - `train_epoch`: Loop once over the train dataset (epoch), perform forward and backward pass, do an optimizer step and don't forget to zero the gradients after you are done. Calculate the average loss and accuracy over the whole training set and save it in the corresponding lists that you find in the base `Trainer` class.\n",
    "> - `val_epoch`: Loop once over the val dataset, perform only the forward pass without gradient tracking (!). Calculate the average loss and accuracy over the whole validation set and save it in the corresponding lists that you find in the base `Trainer` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cee874",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear(2, 1)\n",
    "loss = BinaryCrossEntropyLoss()\n",
    "optim = SGDOptimizer(model, lr=0.001)\n",
    "\n",
    "# Use this the following two lines if you did not manage to\n",
    "# create the Linear and BCE layers:\n",
    "# model = torch.nn.Linear(2, 1)\n",
    "# loss = torch.nn.BCEWithLogitsLoss()\n",
    "# optim = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "trainer = BinaryTrainer(model, loss, (train_data, val_data), optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train for 1000 epochs\n",
    "trainer.fit(epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e0fd7",
   "metadata": {},
   "source": [
    "The following cell should show decreasing loss, if everything is implemented correctly. The train accuracy is 100%, however the validation accuracy stagnates at 98%. You will see in the cell after the next that the reason for this is that the validation data (which we do not use for classification) lies close to the boarder. This effect can be mitigated if more training data is used as then training and validation data should be distributed equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a542be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation curves\n",
    "def plot_results(train, val, title, y_axis):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(train, label=\"train\")\n",
    "    ax.plot(val, label=\"val\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel(y_axis)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(trainer.train_losses, trainer.val_losses, \"Losses\", \"loss\")\n",
    "plot_results(trainer.train_accs, trainer.val_accs, \"Accuracies\", \"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ca893",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "d = [i for i in data]\n",
    "colors = np.array(tuple(\"tab:blue\" if x[1].item() == 0 else \"tab:orange\" for x in data))\n",
    "ax.scatter(np.array([i[0][0].item() for i in data]),\n",
    "           np.array([i[0][1].item() for i in data]), color = colors)\n",
    "\n",
    "x_min, x_max = ax.get_xlim()\n",
    "y_min, y_max = ax.get_ylim()\n",
    "step = 0.1\n",
    "\n",
    "x, y = np.meshgrid(\n",
    "        np.arange(x_min, x_max, step), np.arange(y_min, y_max, step)\n",
    "    )\n",
    "z = (\n",
    "        model.weight[0].detach().numpy() * x\n",
    "        + model.weight[1].detach().numpy() * y\n",
    "        + model.bias.detach().numpy()\n",
    "    )\n",
    "ax.contour(x, y, z, levels=[0], colors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952679fa",
   "metadata": {},
   "source": [
    "## Multiclass\n",
    "Now it is time to also test the multi class case of our implementation (i.e. CrossEntropyLoss). For that we will use a slightly more complex dataset called MNIST. MNIST includes images of handwritten digits (numbers from zero to nine). There is already a Dataset implemented by PyTorch which we can use to download the dataset. In the following cell we also apply a transform to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbab09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST('data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "val = datasets.MNIST('data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "# lets look at the shape of a single sample\n",
    "print(train[0][0].shape)\n",
    "print(f\"Ground truth: {train[0][1]}\")\n",
    "plt.imshow(train[0][0][0])\n",
    "\n",
    "\n",
    "train_data = DataLoader(train, batch_size=32, shuffle=True)\n",
    "val_data = DataLoader(val, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad48f3",
   "metadata": {},
   "source": [
    "> **Task 14** Implement the `train_epoch()` and `val_epoch()` methods of the `CategoricalTrainer` class in the [macrograd/trainer.py](macrograd/trainer.py) file. Hint: Both are very similar to the binary case. Be careful to use one-hot encoding for the ground truth data.\n",
    "> - `train_epoch`: Loop once over the train dataset (epoch), perform forward and backward pass, do an optimizer step and don't forget to zero the gradients after you are done. Calculate the average loss and accuracy over the whole training set and save it in the corresponding lists that you find in the base `Trainer` class.\n",
    "> - `val_epoch`: Loop once over the val dataset, perform only the forward pass without gradient tracking (!). Calculate the average loss and accuracy over the whole validation set and save it in the corresponding lists that you find in the base `Trainer` class.\n",
    "\n",
    "This time we are using a several layer in our neural network, also called a Multi Layer Perceptron (MLP).\n",
    "It is important to add non-linearities (in this case the ReLU function) in between to let the network\n",
    "approximate non-linear functions. Check out how one could also implement the MLP module class below using [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = Linear(input_size, hidden_size)\n",
    "        self.linear2 = Linear(hidden_size, output_size)\n",
    "        self.relu = ReLULayer()\n",
    "\n",
    "        # In case you did not manage to create the Linear\n",
    "        # and ReLU layers use the following lines:\n",
    "        # self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        # self.linear2 = torch.nn.Linear(hidden_size, output_size)\n",
    "        # self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(28*28, 100, 10)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "optim = SGDOptimizer(model, lr=0.01)\n",
    "\n",
    "# In case you did not manage to create the CategoricalCrossEntropyLoss\n",
    "# or SGD optimizer use the following lines:\n",
    "# loss = torch.nn.CrossEntropyLoss()\n",
    "# optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "trainer = CategoricalTrainer(model, loss, (train_data, val_data), optim, 10)\n",
    "# train the network for 10 epochs\n",
    "trainer.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28173ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(trainer.train_losses, trainer.val_losses, \"Losses\", \"loss\")\n",
    "plot_results(trainer.train_accs, trainer.val_accs, \"Accuracies\", \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9f502",
   "metadata": {},
   "source": [
    "The plots above should show that we can achieve even around 90% of accuracy with a very simple neural network and only 5 epochs of training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "de5b31044833cea9ee377c5b8455b5e41e391cda0b415044581a804fd2c5baa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
