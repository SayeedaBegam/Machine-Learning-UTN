{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b1c80e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from random import gauss\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import FloatSlider, interact, Layout, fixed\n",
    "\n",
    "from least_squares import fit_line, line, LSRegression, LSClassification, LSRidgeRegression, phi_poly\n",
    "from least_squares_scikit import ls_regression_scikit\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "width = 9.5\n",
    "plt.rcParams['figure.figsize'] = [width, width / 1.618] \n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913afbc",
   "metadata": {},
   "source": [
    "# Machine Learning Basic Module\n",
    "\n",
    "Florian Walter, Tobias JÃ¼lg, Pierre Krack\n",
    "\n",
    "## General Information About Implementation Assignments\n",
    "We will use the Jupyter Notebook for our implementation exercises. The task description will be provided in the notebook. The code is also run in the notebook. However, the implementation itself is done in additional files which are imported in the notebook. Please do not provide any implementation that you want to be considered for correction in this notebook, but only in Python files in the marked positions. A content of a Python file could for example look similar as shown below:\n",
    "```python\n",
    "def f():\n",
    "    ########################################################################\n",
    "    # YOUR CODE\n",
    "    # TODO: Implement this function\n",
    "    ########################################################################\n",
    "    pass\n",
    "    ########################################################################\n",
    "    # END OF YOUR CODE\n",
    "    ########################################################################\n",
    "```\n",
    "To complete the exercise, remove the `pass` command and only use space inside the `YOUR CODE` block to provide a solution. Other lines within the file may not be changed in order to deliver a valid submission.\n",
    "\n",
    "## General Information About Theory Assignments\n",
    "This Jupyter Notebook also includes a theory assignment. You can either type set your solution in $\\LaTeX$/Word or prepare a digital written or scanned solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfd9aa",
   "metadata": {},
   "source": [
    "## Assignment 2: Least Squares Regression\n",
    "\n",
    "In this assignment you will use the least squares method for regression and classification.\n",
    "We start with the very simplest form to illustrate the concept: you are given a dataset $D$ consisting of pairs $(x, y)$.\n",
    "The problem we want to solve first (linear regression) is, given this dataset and a new value $x \\notin D$, what is the most likely corresponding value $y$.\n",
    "\n",
    "If you want a more concrete example, imagine the dataset contains the height ($x$) and the bone mass ($y$) for 100 unknown individuals.\n",
    "Now the task is, given that dataset, how much bone mass does a two meter tall human have?\n",
    "\n",
    "In principle we could use many different functions with different numbers of parameters to make this prediction, but we keep it simple and just use the equation of a line: $y = mx+b$.\n",
    "\n",
    "Run the next cell to visualize a linear model and its errors.\n",
    "\n",
    "NOTE: The next cell uses the interactive plotting capabilities of matplotlib and [ipywidgets](https://ipywidgets.readthedocs.io/en). Use it to modify $m$ and $b$. It **does not work in VSCode**. If it does not work for you, change the first line from `%matplotlib notebook` to `%matplotlib inline`, then change the last line from `data_x, data_y = interactive_plot(True)` to `data_x, data_y = interactive_plot(False)` You can then change the parameters $m$ and $b$ inside the `interactive_plot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0a4354",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\sayee\\AppData\\Local\\Temp\\ipykernel_15168\\3116917072.py:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  ax.set_title(f\"$m={m:.2f}, y={b:.2f}, \\sum_{{i=0}}^N r_i = {int(ss)}$\")\n",
      "C:\\Users\\sayee\\AppData\\Local\\Temp\\ipykernel_15168\\3116917072.py:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  ax.set_title(f\"$m={m:.2f}, y={b:.2f}, \\sum_{{i=0}}^N r_i = {int(ss)}$\")\n",
      "C:\\Users\\sayee\\AppData\\Local\\Temp\\ipykernel_15168\\3116917072.py:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  ax.set_title(f\"$m={m:.2f}, y={b:.2f}, \\sum_{{i=0}}^N r_i = {int(ss)}$\")\n",
      "C:\\Users\\sayee\\AppData\\Local\\Temp\\ipykernel_15168\\3116917072.py:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  ax.set_title(f\"$m={m:.2f}, y={b:.2f}, \\sum_{{i=0}}^N r_i = {int(ss)}$\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m\n\u001b[0;32m     34\u001b[0m         interact(\n\u001b[0;32m     35\u001b[0m             update,\n\u001b[0;32m     36\u001b[0m             m \u001b[38;5;241m=\u001b[39m FloatSlider(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.001\u001b[39m, value\u001b[38;5;241m=\u001b[39mm, layout\u001b[38;5;241m=\u001b[39mlayout),\n\u001b[0;32m     37\u001b[0m             b \u001b[38;5;241m=\u001b[39m FloatSlider(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m80\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, value\u001b[38;5;241m=\u001b[39mb, layout\u001b[38;5;241m=\u001b[39mlayout),\n\u001b[0;32m     38\u001b[0m         )\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_x, data_y\n\u001b[1;32m---> 40\u001b[0m data_x, data_y \u001b[38;5;241m=\u001b[39m \u001b[43minteractive_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m, in \u001b[0;36minteractive_plot\u001b[1;34m(interactive)\u001b[0m\n\u001b[0;32m     18\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     19\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 20\u001b[0m data_x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m rng\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     21\u001b[0m data_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.1\u001b[39m \u001b[38;5;241m*\u001b[39m data_x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m50.0\u001b[39m \u001b[38;5;241m+\u001b[39m rng\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     22\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "def plot(ax, data_x, data_y, m, b):\n",
    "    N = len(data_x)\n",
    "    ax.scatter(data_x, data_y, label=\"data\")\n",
    "    residuals = np.zeros((N, 2, 2))\n",
    "    residuals[:,0,:] = np.stack((data_x, data_y), axis=1)\n",
    "    residuals[:,1,:] = np.stack((data_x, m * data_x + b), axis=1)\n",
    "    residual_lines = ax.add_collection(LineCollection(residuals, linestyles=\"dashed\", colors=\"tab:orange\", label=\"residuals\"))\n",
    "    prediction_line, = ax.plot(data_x, line(data_x, m, b), label=\"predictions\")\n",
    "    ss = np.sum(np.square(residuals[:,0,1] - residuals[:,1,1]))\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"$m={m:.2f}, y={b:.2f}, \\sum_{{i=0}}^N r_i = {int(ss)}$\")\n",
    "    return residual_lines, prediction_line, residuals, ss\n",
    "\n",
    "\n",
    "def interactive_plot(interactive: bool):\n",
    "    m = 0.5\n",
    "    b = 1\n",
    "    N = 100\n",
    "    data_x = np.arange(0, 100, 1) + rng.normal(loc=0, scale=0.2, size=100)\n",
    "    data_y = 1.1 * data_x - 50.0 + rng.normal(loc=0, scale=20, size=100)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    residual_lines, prediction_line, residuals, ss = plot(ax, data_x, data_y, m, b)\n",
    "    def update(m, b):\n",
    "        residuals[:,0,:] = np.stack((data_x, data_y), axis=1)\n",
    "        residuals[:,1,:] = np.stack((data_x, m * data_x + b), axis=1)\n",
    "        residual_lines.set_segments(residuals)\n",
    "        prediction_line.set_ydata(m * data_x + b)\n",
    "        ss = np.sum(np.square(residuals[:,0,1] - residuals[:,1,1]))\n",
    "        ax.set_title(f\"$m={m:.2f}, y={b:.2f}, \\sum_{{i=0}}^N r_i = {int(ss)}$\")\n",
    "    layout = Layout(width=\"900px\")\n",
    "    if interactive:\n",
    "        interact(\n",
    "            update,\n",
    "            m = FloatSlider(min=-1.5, max=1.5, step=.001, value=m, layout=layout),\n",
    "            b = FloatSlider(min=-80, max=80, step=0.001, value=b, layout=layout),\n",
    "        )\n",
    "    return data_x, data_y\n",
    "data_x, data_y = interactive_plot(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a0730",
   "metadata": {},
   "source": [
    "As you may have already guessed, we will now do the \"learning\" part of machine learning, by automatically finding the best values for $m$ and $b$.\n",
    "\n",
    "If we make the length of the orange dotted lines (the residuals) as small as possible we find the so-called best-fit line.\n",
    "Note that we cannot just minimize the sum of the residuals because the residuals can be negative: we want the sum of the residuals to go towards zero and not towards $-\\infty$.\n",
    "\n",
    "So we want to miminize *the length* of the red dotted lines, i.e. the asolute values of the residuals:\n",
    "$$\\sum_{i=0}^{\\texttt{N}} |r_i|\\textrm{,}$$\n",
    "where $r_i = y_i - m \\cdot x_i + b$ is the difference between the actual and the predicted data point (the error, in linear regression a.k.a. the residuals), and $N$ is the number of samples in our dataset.\n",
    "\n",
    "How do we do that?\n",
    "Typically, and you will notice that this is a pattern in machine learning, we want to take the derivative of our error (here the residuals) and set it equal to zero.\n",
    "That is, find the point where the error is minimized.\n",
    "Great, so we just want to derive $\\sum_{i=0}^{N} |r_i|$ with respect to the parameters of our model, $m$ and $b$.\n",
    "Unfortunately the absolute function is not differentiable, so we need to find another function such that, when the output of the function is minimized then the input is close to zero.\n",
    "A simple function that achieves this is the square function:\n",
    "$$f(x): \\mathbb{R} \\mapsto \\mathbb{R}_{+} = x^2$$\n",
    "\n",
    "We can now derive with respect to the parameters of the model.\n",
    "When we write $\\sum$ we mean $\\sum_{i=0}^N$.\n",
    "Try to follow what we do: it is, very simple math that you all know.\n",
    "Don't be afraid by the length of it, it is just because we wrote out every single step, other online resources compress many of these steps.\n",
    "$$\n",
    "\\begin{split}\n",
    "&\\frac{\\textrm{d}}{\\textrm{d}m} \\sum \\big(y_i - (mx_i + b)\\big)^2 = 0 \\\\\n",
    "\\textrm{(chain-rule)} \\Leftrightarrow\\,&\\sum 2\\,(y_i - mx_i - b)(-x_i) =0 \\\\\n",
    "\\Leftrightarrow\\,&\\sum (y_i - mx_i - b)\\,x_i = 0 \\\\\n",
    "\\Leftrightarrow\\,&\\sum y_ix_i - mx_i^2 - bx_i = 0 \\\\\n",
    "\\Leftrightarrow\\,&m\\sum x_i^2 b\\sum x_i = \\sum x_iy_i\n",
    "\\end{split}\n",
    "\\qquad\n",
    "\\begin{split}\n",
    "&\\frac{\\textrm{d}}{\\textrm{d}b} \\sum \\big(y_i - (mx_i + b)\\big)^2 = 0 \\\\\n",
    "\\textrm{(chain-rule)}\\Leftrightarrow\\,&\\sum 2\\,(y_i - mx_i - b)(-1) = 0 \\\\\n",
    "\\Leftrightarrow\\,&\\sum y_i - mx_i - b = 0 \\\\\n",
    "\\Leftrightarrow\\,&\\sum y_i -m\\sum x_i -Nb = 0 \\\\\n",
    "\\Leftrightarrow\\,&m\\sum x_i + bN = \\sum y_i\n",
    "\\end{split}\n",
    "$$\n",
    "Now we our two equations have a nice form for solving in a system of linear equations!\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\sum x_i^2 & \\sum x_i \\\\\n",
    "\\sum x_i & N\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "m \\\\\n",
    "b\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\sum x_iy_i\\\\\n",
    "\\sum y_i\n",
    "\\end{bmatrix}\\\\\n",
    "\\begin{bmatrix}\n",
    "m \\\\\n",
    "b\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\sum x_i^2 & \\sum x_i \\\\\n",
    "\\sum x_i & N\n",
    "\\end{bmatrix}^{-1} \\cdot\n",
    "\\begin{bmatrix}\n",
    "\\sum x_iy_i\\\\\n",
    "\\sum y_i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Using the formula $\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}^{-1} = \\frac{1}{ad - bc}\\begin{bmatrix}d & -b\\\\-c&a\\end{bmatrix}$, we have:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "m \\\\ b\n",
    "\\end{bmatrix} = \\frac{1}{\\sum x_i^2 n - \\sum x_i \\sum x_i}\n",
    "\\begin{bmatrix}\n",
    "N & -\\sum x_i \\\\\n",
    "-\\sum x_i & \\sum x_i^2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sum x_iy_i \\\\ \\sum y_i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "And finally we can write:\n",
    "$$\n",
    "m = \\frac{N \\sum x_iy_i - \\sum x_i \\sum y_i}{N\\sum x_i^2-\\sum x_i\\sum x_i}\\textrm{,}\\quad\n",
    "b = \\frac{-\\sum x_i \\sum x_iy_i + \\sum x_i^2 \\sum y_i}{N\\sum x_i^2-\\sum x_i\\sum x_i}\n",
    "$$\n",
    "\n",
    "> **Task 1** Open [`least_squares.py`](./least_squares.py) and implement `line()` and `fit_line()` using the formula derived above for $m$ and $b$. Then run the next cell to see whether your code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60819d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "m, b = fit_line(data_x, data_y)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "residual_lines, prediction_line, residuals, ss = plot(ax, data_x, data_y, m, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989a4f0",
   "metadata": {},
   "source": [
    "## More Dimensions and the Ridge regression\n",
    "In the last section, we handled the simple special case where a single numeric value is assigned a number (remember: predict weight $y$ based on height $x$).\n",
    "Typically however, $y$ will depend on multiple values.\n",
    "Additionaly the data might not follow a straight line but some polynomial curve.\n",
    "\n",
    "Fortunately, there is a solution to this: we can derive the following error formula, set it to zero and solve for $w$:\n",
    "$$E(w) = \\frac{1}{2} \\sum_{i=1}^{N} (w^T \\phi(x_i) - y_i)^2$$\n",
    "Where this time $w$ is a vector and $\\phi$ is a so-called basis function.\n",
    "In the simple line example above $w$ would be the vector $\\begin{pmatrix}m\\\\b\\end{pmatrix}$ and $\\phi(x) = \\begin{pmatrix}x\\\\1\\end{pmatrix}$ such that $w^T\\phi(x_i) = (m, b)\\cdot\\begin{pmatrix}x\\\\1\\end{pmatrix} = mx+b$.\n",
    "Choose $\\phi(x) = \\begin{pmatrix}x^2\\\\1\\end{pmatrix}$ and you can find a good fit for a dataset shaped like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1303db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**2\n",
    "x = np.linspace(-1, 1, 100)\n",
    "y = f(np.linspace(-2, 2, 100)) + np.random.random(100) * - 0.5\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35067f5c",
   "metadata": {},
   "source": [
    "Following this scheme you can find a good fit regardless of the dimension of your dataset and regardless of its shape, proviced you choose the right basis functions.\n",
    "In the slides there is the derivation for finding the error-minimizing weights in this general case.\n",
    "\n",
    "Now it is your turn, adapt the derivation shown in class to the Ridge regression! \n",
    "\n",
    "> **Task 2** Follow that example to derive a closed form solution for the equation $\\nabla_w E_{\\textrm{ridge}}(w) = 0$\n",
    "Reminder: The ridge regression error function looks like this:\n",
    "$$E_{\\text{ridge}}(w) = \\frac{1}{2} \\sum_{i=1}^{N} (w^T \\phi(x_i) - y_i)^2 + \\frac{\\lambda}{2} w^T w$$\n",
    "where $x_i\\in\\mathbb{R}^{D}$ is the input vector, $\\phi: \\mathbb{R}^D \\rightarrow \\mathbb{R}^M$ the basis functions, $w\\in\\mathbb{R}^M$ the weights and $y_i\\in\\mathbb{R}$ are the regression labels.\n",
    "The dataset contains $N$ samples.\n",
    ">\n",
    ">Hint: You can use the design matrix introduced in the course: $\\Phi \\in\\mathbb{R}^{N\\times M}$\n",
    "$$\n",
    "\\Phi = \\begin{bmatrix}\n",
    "\\phi_1(x_1) & \\phi_2(x_1) & \\dots & \\phi_M(x_1) \\\\\n",
    "\\vdots &\\vdots & \\ddots &\\vdots  \\\\\n",
    "\\phi_1(x_N) & \\phi_2(x_N) & \\dots & \\phi_M(x_N) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $\\phi_j(x_i): \\mathbb{R}^D \\rightarrow \\mathbb{R}$ is the j-th component of the $\\phi$ function.\n",
    ">\n",
    ">The following steps may be used as a guidance for the derivation:\n",
    ">1. Write down the error term for the ridge regression in matrix notation using the $\\Phi$ matrix.\n",
    ">2. Calculate the gradient from the error function $\\nabla_w E_{\\text{ridge}}(w)$. Hint: You might use Equations (69) and (81) from the [Matrixcookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) for the derivation if you are new to vector analysis.\n",
    ">3. Set the gradient to zero and solve for $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d2fbad",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Regression Special Case of Ridge Regression\n",
    "Show that ridge regression on a design matrix $\\Phi\\in\\mathbb{R}^{N\\times M}$ with regularization strength $\\lambda$ is equivalent to ordinary least squares regression with an augmented design matrix and target vector\n",
    "$$\n",
    "    \\hat{\\Phi} = \\begin{pmatrix} \\Phi \\sqrt{\\lambda} \\pmb{I}_M \\end{pmatrix} \\quad \\text{and} \\quad \\hat{y} = \\begin{pmatrix} y \\pmb{0}_M \\end{pmatrix}\\text{, }\n",
    "$$\n",
    "where $\\pmb{I}_M$ is the identity matrix of size $M$ and $\\pmb{0}_M$ is the zero vector of size $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d6f8f5",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Now that we derived the second more general equation, we can implement it using efficient parallel numpy functions, which map nicely to the equations derived above\n",
    "> **Task 3** Open [`least_squares.py`](./least_squares.py) and implement the missing parts of the `LSRegression`. Then look at the cell below and understand how we generate the data (in the line `z = (...)`), then go back to [`least_squares.py`](./least_squares.py) and implement an appropriate `phy_poly` function.\n",
    "Using the convenient and efficient functions of numpy all the missing components can be implemented as simple one-liners.\n",
    "NOTE: you are not allowed to use `np.linalg.lstsq`, neither are you allowed to use `np.linalg.pinv`.\n",
    "\n",
    "Run the next cell to see if your implementation works. The orange surface shows the prediction of the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c252707",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LSRegression(dim=2, m=3, phi=phi_poly)\n",
    "x, y = np.mgrid[-10:10:5j, -10:10:5j]\n",
    "z = (2*x**2 - 2*y**2 + 2.71828) + np.random.normal(scale=10, size=x.shape)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(x, y, z, color=\"black\", label=\"data\")\n",
    "data = np.stack([x.ravel(), y.ravel()], 1)\n",
    "reg.fit(data, z.ravel())\n",
    "x, y = np.mgrid[-15:15:25j, -15:15:25j]\n",
    "ax.plot_surface(x, y, reg.predict(np.stack([x.ravel(), y.ravel()], 1)).reshape((25, 25)), alpha=0.7, cmap=cm.Oranges)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb52077",
   "metadata": {},
   "source": [
    "> **Task 4** Implement the ridge regression by filling in the missing parts in the `LSRidgeRegression` class inside the [`least_squares.py`](least_squares.py) file. Then run the next cell to see the difference between the basic and the ridge regression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b00d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x = np.linspace(-1, 1, n).reshape((n, 1))\n",
    "y = 2 * x + 1 + rng.normal(scale=0.2, size=(n,1))\n",
    "y.reshape((n,1))\n",
    "degree = 10\n",
    "phi = lambda x: np.array([x[0]**n for n in range(degree)])\n",
    "\n",
    "reg = LSRegression(dim=1, m=degree, phi=phi)\n",
    "reg.fit(x, y)\n",
    "ridgereg = LSRidgeRegression(dim=1, m=degree, phi=phi)\n",
    "ridgereg.fit(x, y, lmbda=0.1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1,2)\n",
    "ax[0].scatter(x, y)\n",
    "ax[1].scatter(x, y)\n",
    "x = np.linspace(-1.5, 1.5, 100).reshape(100,1)\n",
    "y = reg.predict(x)\n",
    "ax[0].plot(x, y)\n",
    "ax[0].set_ylim(-2, 4)\n",
    "y = ridgereg.predict(x)\n",
    "ax[1].plot(x, y)\n",
    "ax[1].set_ylim(-2, 4)\n",
    "plt.show()\n",
    "print(f\"Normal regression: w = (\" + \", \".join(f\"{w:.2f}\" for w in reg.weights.flatten()) + \")\")\n",
    "print(f\"Ridge regression: w = (\" + \", \".join(f\"{w:.2f}\" for w in ridgereg.weights.flatten()) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8db6a",
   "metadata": {},
   "source": [
    "# Least Squares Classification\n",
    "\n",
    "Perhaps surprisingly, we can use the least squares method for classification.\n",
    "\n",
    "> **Task 5** implement the missing parts of the `LSClassification` class inside the [`least_squares.py`](least_squares.py) file. Use the Bishop book chapter 4.1.3 for guidance. Use the next cell to plot the predictions of your linear classifier and verify if your implementation is correct. The points on the bottom left should be blue, top left green and top right orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate(\n",
    "    (\n",
    "        rng.normal(loc=-1, scale=0.2, size=(50, 2)),\n",
    "        rng.normal(loc=1, scale=0.2, size=(50, 2)),\n",
    "        np.stack(\n",
    "            (\n",
    "                rng.normal(loc=-1, scale=0.2, size=50),\n",
    "                rng.normal(loc=1.0, scale=0.2, size=50)\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "labels = np.zeros((150, 3))\n",
    "labels[:50,0] = 1\n",
    "labels[50:100,1] = 1\n",
    "labels[100:150,2] = 1\n",
    "\n",
    "classifier = LSClassification(dim=2, k=3)\n",
    "classifier.fit(data, labels)\n",
    "colors = [(\"tab:blue\", \"tab:orange\", \"tab:green\")[p] for p in classifier.predict(data)]\n",
    "plt.scatter(data[:,0], data[:,1], color = colors)\n",
    "x = classifier.augment(np.array([[-1.5, -1.5],[1.5,1.5]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951e4ed",
   "metadata": {},
   "source": [
    "# Real data and scikit-learn\n",
    "Like in the first exercise we will now use real data along with [pandas](https://pandas.pydata.org) and [scikit-learn](https://scikit-learn.org) to illustrate a real world workflow. We use the recently published [weather prediction dataset](https://zenodo.org/records/7525955) specifically designed for teaching. It is \"complex enough to demonstrate realistic issues such as overfitting and unbalanced data, while still remaining intuitively accessible.\" It contains data collected from 2000 to 2010 at the following weather stations:\n",
    "![WEATHER_PREDICTION_MAP](weather_prediction_dataset_map.png)\n",
    "\n",
    "Because the pandas data manipulation code is a bit more involved (multi indexing is part of the [advanced ](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html) section of the user guide), we will do that for you. Try to follow along. You can type `display(df)` at any point in the following code cell if you want to see the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and remove the \"MONTH\" column, we don't need it.\n",
    "df = pd.read_csv(\"./weather_prediction_dataset.csv\").drop(\"MONTH\", axis=1)\n",
    "print(\"Before modifying the dataframe:\")\n",
    "display(df.head(10))\n",
    "# Transform the \"DATE\" column into a pandas time series object and use it as the index of the dataframe.\n",
    "# This enables things like 'data.index.month' as seen below\n",
    "df.index = pd.to_datetime(df.pop(\"DATE\"), format=\"%Y%m%d\")\n",
    "# This snippet was taken from https://github.com/pandas-dev/pandas/issues/7454\n",
    "# For example,'DJF' means december january february. This is apparently how seasons are often named in geosciences.\n",
    "# It creates the following array: ['DJF' 'DJF' 'MAM' 'MAM' 'MAM' 'JJA' 'JJA' 'JJA' 'SON' 'SON' 'SON' 'DJF']\n",
    "SEASONS = np.array(['DJF', 'MAM', 'JJA', 'SON'])\n",
    "month = np.arange(12) + 1\n",
    "season = SEASONS[(month // 3) % 4]\n",
    "###\n",
    "# Here we do multi-level indexing, we compute the mean per year and month\n",
    "df = df.groupby([(df.index.year), (season[df.index.month-1])]).mean()\n",
    "print(\"After modifying the dataframe:\")\n",
    "display(df.head(10))\n",
    "# Now we select the mean minimal temperature in winter and mean maximal temperature in summer\n",
    "min_temp_winter = df.xs(\"DJF\", level=1).filter(regex=r\".*_temp_min\").mean(axis=1)\n",
    "max_temp_summer = df.xs(\"JJA\", level=1).filter(regex=r\".*_temp_max\").mean(axis=1)\n",
    "# Reshape it for scikit\n",
    "X_data = min_temp_winter.index.to_numpy().reshape(-1,1)\n",
    "print(\"Min temp winter\") \n",
    "print(min_temp_winter)\n",
    "print(\"Max temp summer\")\n",
    "print(max_temp_summer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648174a",
   "metadata": {},
   "source": [
    "Now we have the mean minimal temperatures in winter and the mean maximal temperatures in summer, i.e. the extreme weather in europe. You will now use this data to fit a linear regression model using [scikit-learn](https://scikit-learn.org). The you will use the fitted model to predict the weather in the future.\n",
    "\n",
    "> **Task 6** Open [`ls_regression_scikit.py`](./ls_regression_scikit.py) and implement the function 'ls_regression_scikit' using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b7ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_until = 2050\n",
    "winter_predictions = ls_regression_scikit(X=X_data, y=min_temp_winter.values, predict_until=predict_until)\n",
    "summer_predictions = ls_regression_scikit(X=X_data[:-1], y=max_temp_summer.values, predict_until=predict_until)\n",
    "x = np.arange(1999, predict_until, 1).reshape(-1, 1)\n",
    "plt.scatter(X_data, min_temp_winter.values)\n",
    "plt.plot(\n",
    "    x,    \n",
    "    winter_predictions,\n",
    "    label=\"Mean min temperature in winter\"\n",
    ")\n",
    "plt.scatter(max_temp_summer.index, max_temp_summer.values)\n",
    "plt.plot(\n",
    "    x,\n",
    "    summer_predictions,\n",
    "    label=\"Mean max temperature in summer\"\n",
    ")\n",
    "plt.title(\"Extreme temperatures in Europe\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Temperature (Â°C)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61a922",
   "metadata": {},
   "source": [
    "> **Task 7** Discuss on Canvas: Is it reasonable to predict the weather as we did above? Why / why not?\n",
    "As a hint, try predicting the weather 1000 years from now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
